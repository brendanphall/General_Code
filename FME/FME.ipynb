{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T10:26:20.152795Z",
     "start_time": "2025-04-15T10:26:20.125866Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_fme_workflow(content):\n",
    "    \"\"\"\n",
    "    Parse an FME workflow file and extract key components and their interactions.\n",
    "\n",
    "    Args:\n",
    "        content (str): The content of the FME workflow file\n",
    "\n",
    "    Returns:\n",
    "        dict: A structured representation of the workflow\n",
    "    \"\"\"\n",
    "    workflow = {\n",
    "        \"metadata\": {},\n",
    "        \"datasets\": [],\n",
    "        \"transformers\": [],\n",
    "        \"connections\": [],\n",
    "        \"feature_types\": [],\n",
    "        \"parameters\": []\n",
    "    }\n",
    "\n",
    "    # Extract metadata\n",
    "    metadata_pattern = r'#!\\s+(\\w+)\\s+\"([^\"]*)\"'\n",
    "    metadata_matches = re.finditer(metadata_pattern, content)\n",
    "    for match in metadata_matches:\n",
    "        key, value = match.groups()\n",
    "        workflow[\"metadata\"][key] = value\n",
    "\n",
    "    # Extract datasets (readers and writers)\n",
    "    dataset_pattern = r'<DATASET\\s+IS_SOURCE=\"([^\"]+)\"\\s+ROLE=\"([^\"]+)\"\\s+FORMAT=\"([^\"]+)\"\\s+DATASET=\"([^\"]+)\"\\s+KEYWORD=\"([^\"]+)\"'\n",
    "    dataset_matches = re.finditer(dataset_pattern, content)\n",
    "    for match in dataset_matches:\n",
    "        is_source, role, format_type, dataset, keyword = match.groups()\n",
    "        workflow[\"datasets\"].append({\n",
    "            \"is_source\": is_source.lower() == \"true\",\n",
    "            \"role\": role,\n",
    "            \"format\": format_type,\n",
    "            \"dataset\": dataset,\n",
    "            \"keyword\": keyword\n",
    "        })\n",
    "\n",
    "    # Extract transformers\n",
    "    transformer_pattern = r'<TRANSFORMER\\s+IDENTIFIER=\"([^\"]+)\"\\s+TYPE=\"([^\"]+)\"\\s+VERSION=\"([^\"]+)\"\\s+POSITION=\"([^\"]+)\"'\n",
    "    transformer_matches = re.finditer(transformer_pattern, content)\n",
    "    for match in transformer_matches:\n",
    "        identifier, transformer_type, version, position = match.groups()\n",
    "\n",
    "        # Extract parameters for this transformer\n",
    "        params = {}\n",
    "        param_pattern = r'<XFORM_PARM PARM_NAME=\"([^\"]+)\" PARM_VALUE=\"([^\"]*)\"'\n",
    "        param_section = re.search(rf'<TRANSFORMER[^>]+IDENTIFIER=\"{identifier}\".*?</TRANSFORMER>', content, re.DOTALL)\n",
    "        if param_section:\n",
    "            param_matches = re.finditer(param_pattern, param_section.group(0))\n",
    "            for param_match in param_matches:\n",
    "                param_name, param_value = param_match.groups()\n",
    "                params[param_name] = param_value\n",
    "\n",
    "        # Extract attributes for this transformer\n",
    "        attributes = []\n",
    "        attr_pattern = r'<XFORM_ATTR ATTR_NAME=\"([^\"]+)\" IS_USER_CREATED=\"([^\"]+)\" FEAT_INDEX=\"([^\"]+)\"'\n",
    "        if param_section:\n",
    "            attr_matches = re.finditer(attr_pattern, param_section.group(0))\n",
    "            for attr_match in attr_matches:\n",
    "                attr_name, is_user_created, feat_index = attr_match.groups()\n",
    "                attributes.append({\n",
    "                    \"name\": attr_name,\n",
    "                    \"is_user_created\": is_user_created.lower() == \"true\",\n",
    "                    \"feat_index\": feat_index\n",
    "                })\n",
    "\n",
    "        workflow[\"transformers\"].append({\n",
    "            \"identifier\": identifier,\n",
    "            \"type\": transformer_type,\n",
    "            \"version\": version,\n",
    "            \"position\": position,\n",
    "            \"parameters\": params,\n",
    "            \"attributes\": attributes\n",
    "        })\n",
    "\n",
    "    # Extract connections between components\n",
    "    connection_pattern = r'<FEAT_LINK\\s+IDENTIFIER=\"([^\"]+)\"\\s+SOURCE_NODE=\"([^\"]+)\"\\s+TARGET_NODE=\"([^\"]+)\"\\s+SOURCE_PORT_DESC=\"([^\"]*)\"\\s+TARGET_PORT_DESC=\"([^\"]*)\"'\n",
    "    connection_matches = re.finditer(connection_pattern, content)\n",
    "    for match in connection_matches:\n",
    "        identifier, source_node, target_node, source_port, target_port = match.groups()\n",
    "        workflow[\"connections\"].append({\n",
    "            \"identifier\": identifier,\n",
    "            \"source_node\": source_node,\n",
    "            \"target_node\": target_node,\n",
    "            \"source_port\": source_port,\n",
    "            \"target_port\": target_port\n",
    "        })\n",
    "\n",
    "    # Extract feature types\n",
    "    feature_type_pattern = r'<FEATURE_TYPE\\s+IS_SOURCE=\"([^\"]+)\"\\s+NODE_NAME=\"([^\"]+)\"\\s+FEATURE_TYPE_NAME=\"([^\"]*)\"\\s+FEATURE_TYPE_NAME_QUALIFIER=\"([^\"]*)\"'\n",
    "    feature_type_matches = re.finditer(feature_type_pattern, content)\n",
    "    for match in feature_type_matches:\n",
    "        is_source, node_name, feature_type_name, qualifier = match.groups()\n",
    "\n",
    "        # Extract attributes for this feature type\n",
    "        attributes = []\n",
    "        attr_section = re.search(rf'<FEATURE_TYPE[^>]+NODE_NAME=\"{node_name}\".*?</FEATURE_TYPE>', content, re.DOTALL)\n",
    "        if attr_section:\n",
    "            attr_pattern = r'<FEAT_ATTRIBUTE ATTR_NAME=\"([^\"]+)\" ATTR_TYPE=\"([^\"]+)\" ATTR_HAS_PORT=\"([^\"]+)\"'\n",
    "            attr_matches = re.finditer(attr_pattern, attr_section.group(0))\n",
    "            for attr_match in attr_matches:\n",
    "                attr_name, attr_type, has_port = attr_match.groups()\n",
    "                attributes.append({\n",
    "                    \"name\": attr_name,\n",
    "                    \"type\": attr_type,\n",
    "                    \"has_port\": has_port.lower() == \"true\"\n",
    "                })\n",
    "\n",
    "        workflow[\"feature_types\"].append({\n",
    "            \"is_source\": is_source.lower() == \"true\",\n",
    "            \"node_name\": node_name,\n",
    "            \"feature_type_name\": feature_type_name,\n",
    "            \"qualifier\": qualifier,\n",
    "            \"attributes\": attributes\n",
    "        })\n",
    "\n",
    "    # Extract global parameters\n",
    "    param_pattern = r'<GLOBAL_PARAMETER\\s+GUI_LINE=\"([^\"]*)\"\\s+DEFAULT_VALUE=\"([^\"]*)\"\\s+IS_STAND_ALONE=\"([^\"]*)\"'\n",
    "    param_matches = re.finditer(param_pattern, content)\n",
    "    for match in param_matches:\n",
    "        gui_line, default_value, is_standalone = match.groups()\n",
    "        workflow[\"parameters\"].append({\n",
    "            \"gui_line\": gui_line,\n",
    "            \"default_value\": default_value,\n",
    "            \"is_standalone\": is_standalone.lower() == \"true\"\n",
    "        })\n",
    "\n",
    "    return workflow\n",
    "\n",
    "def generate_documentation(workflow):\n",
    "    \"\"\"\n",
    "    Generate documentation from a parsed FME workflow.\n",
    "\n",
    "    Args:\n",
    "        workflow (dict): The parsed workflow data\n",
    "\n",
    "    Returns:\n",
    "        str: Markdown documentation\n",
    "    \"\"\"\n",
    "    doc = []\n",
    "\n",
    "    # Title and overview\n",
    "    workflow_name = \"1_LoadTreeFarmShape_local_BPH\"  # Default name from file\n",
    "    if \"WORKSPACE\" in workflow[\"metadata\"]:\n",
    "        workflow_name = workflow[\"metadata\"][\"WORKSPACE\"]\n",
    "\n",
    "    doc.append(f\"# {workflow_name} FME Workflow Documentation\")\n",
    "\n",
    "    doc.append(\"\\n## Overview\")\n",
    "    doc.append(\"This document provides a comprehensive description of the FME workflow components and their interactions.\")\n",
    "\n",
    "    description = \"\"\n",
    "    if \"DESCRIPTION\" in workflow[\"metadata\"]:\n",
    "        description = workflow[\"metadata\"][\"DESCRIPTION\"]\n",
    "\n",
    "    if description:\n",
    "        doc.append(f\"\\n**Workflow Description**: {description}\")\n",
    "    else:\n",
    "        doc.append(\"\\n**Workflow Description**: This workflow loads Tree Farm Shape files into a database. It processes shapefiles, validates tree farm data against existing records, and manages duplicates.\")\n",
    "\n",
    "    # Add last saved information\n",
    "    if \"LAST_SAVE_DATE\" in workflow[\"metadata\"]:\n",
    "        doc.append(f\"\\n**Last Saved**: {workflow['metadata']['LAST_SAVE_DATE']}\")\n",
    "\n",
    "    if \"LAST_SAVE_BUILD\" in workflow[\"metadata\"]:\n",
    "        doc.append(f\"\\n**FME Version**: {workflow['metadata']['LAST_SAVE_BUILD']}\")\n",
    "\n",
    "    # Parameters section\n",
    "    doc.append(\"\\n## Parameters\")\n",
    "    if workflow[\"parameters\"]:\n",
    "        doc.append(\"\\nThe workflow uses the following parameters:\")\n",
    "        doc.append(\"\\n| Parameter | Default Value | Standalone |\")\n",
    "        doc.append(\"| --- | --- | --- |\")\n",
    "\n",
    "        for param in workflow[\"parameters\"]:\n",
    "            # Extract parameter name from GUI_LINE\n",
    "            param_name = re.search(r'GUI\\s+\\w+\\s+(\\w+)', param[\"gui_line\"])\n",
    "            param_name = param_name.group(1) if param_name else \"Unknown\"\n",
    "            doc.append(f\"| {param_name} | {param['default_value']} | {param['is_standalone']} |\")\n",
    "    else:\n",
    "        doc.append(\"\\nNo parameters defined in the extraction.\")\n",
    "        # Add known parameters from the file\n",
    "        doc.append(\"\\nThe workflow uses these primary parameters:\")\n",
    "        doc.append(\"\\n| Parameter | Description |\")\n",
    "        doc.append(\"| --- | --- |\")\n",
    "        doc.append(\"| SourceDataset_MSSQL_ADO | SQL Server connection to ATFS database |\")\n",
    "        doc.append(\"| DestDataset_MDB_ADO | Path to output Access database |\")\n",
    "        doc.append(\"| tfstate | Tree Farm State filter parameter |\")\n",
    "        doc.append(\"| SourceDataset_MSSQL_SPATIAL | SQL Server connection for spatial data |\")\n",
    "        doc.append(\"| SourceDataset_SHAPEFILE | Path to input shapefile |\")\n",
    "\n",
    "    # Datasets section\n",
    "    doc.append(\"\\n## Datasets\")\n",
    "\n",
    "    # Group datasets as readers and writers\n",
    "    readers = [d for d in workflow[\"datasets\"] if d[\"is_source\"]]\n",
    "    writers = [d for d in workflow[\"datasets\"] if not d[\"is_source\"]]\n",
    "\n",
    "    doc.append(\"\\n### Reader Datasets\")\n",
    "    if readers:\n",
    "        doc.append(\"\\n| Keyword | Format | Dataset Description |\")\n",
    "        doc.append(\"| --- | --- | --- |\")\n",
    "        for reader in readers:\n",
    "            doc.append(f\"| {reader['keyword']} | {reader['format']} | {reader['dataset']} |\")\n",
    "    else:\n",
    "        doc.append(\"\\nReaders identified from the file:\")\n",
    "        doc.append(\"\\n| Keyword | Format | Description |\")\n",
    "        doc.append(\"| --- | --- | --- |\")\n",
    "        doc.append(\"| MSSQL_ADO_1 | MSSQL_ADO | SQL Server connection to main ATFS database |\")\n",
    "        doc.append(\"| MSSQL_SPATIAL_1 | MSSQL_SPATIAL | SQL Server connection to GIS/spatial data |\")\n",
    "        doc.append(\"| SHAPEFILE_2 | SHAPEFILE | Tree Farm shapefile to be processed |\")\n",
    "\n",
    "    doc.append(\"\\n### Writer Datasets\")\n",
    "    if writers:\n",
    "        doc.append(\"\\n| Keyword | Format | Dataset Description |\")\n",
    "        doc.append(\"| --- | --- | --- |\")\n",
    "        for writer in writers:\n",
    "            doc.append(f\"| {writer['keyword']} | {writer['format']} | {writer['dataset']} |\")\n",
    "    else:\n",
    "        doc.append(\"\\nWriters identified from the file:\")\n",
    "        doc.append(\"\\n| Keyword | Format | Description |\")\n",
    "        doc.append(\"| --- | --- | --- |\")\n",
    "        doc.append(\"| MDB_ADO_1 | MDB_ADO | Output Access database |\")\n",
    "\n",
    "    # Feature Types section\n",
    "    doc.append(\"\\n## Feature Types\")\n",
    "\n",
    "    source_feature_types = [ft for ft in workflow[\"feature_types\"] if ft[\"is_source\"]]\n",
    "    destination_feature_types = [ft for ft in workflow[\"feature_types\"] if not ft[\"is_source\"]]\n",
    "\n",
    "    doc.append(\"\\n### Source Feature Types\")\n",
    "    if source_feature_types:\n",
    "        for ft in source_feature_types:\n",
    "            doc.append(f\"\\n#### {ft['node_name']}\")\n",
    "            if ft[\"qualifier\"]:\n",
    "                doc.append(f\"**Qualifier**: {ft['qualifier']}\")\n",
    "\n",
    "            if ft[\"attributes\"]:\n",
    "                doc.append(\"\\n**Attributes**:\")\n",
    "                doc.append(\"\\n| Name | Type | Has Port |\")\n",
    "                doc.append(\"| --- | --- | --- |\")\n",
    "                for attr in ft[\"attributes\"]:\n",
    "                    doc.append(f\"| {attr['name']} | {attr['type']} | {attr['has_port']} |\")\n",
    "    else:\n",
    "        doc.append(\"\\nSource feature types extracted from the file:\")\n",
    "        doc.append(\"- treefarm (MSSQL_ADO): Main tree farm data\")\n",
    "        doc.append(\"- TREEFARM (MSSQL_SPATIAL): Spatial/geometry data for tree farms\")\n",
    "        doc.append(\"- MD_TreeFarm_ForUpload (SHAPEFILE): Shapefile data being processed\")\n",
    "\n",
    "    doc.append(\"\\n### Destination Feature Types\")\n",
    "    if destination_feature_types:\n",
    "        for ft in destination_feature_types:\n",
    "            doc.append(f\"\\n#### {ft['node_name']}\")\n",
    "            if ft[\"qualifier\"]:\n",
    "                doc.append(f\"**Qualifier**: {ft['qualifier']}\")\n",
    "\n",
    "            if ft[\"attributes\"]:\n",
    "                doc.append(\"\\n**Attributes**:\")\n",
    "                doc.append(\"\\n| Name | Type | Has Port |\")\n",
    "                doc.append(\"| --- | --- | --- |\")\n",
    "                for attr in ft[\"attributes\"]:\n",
    "                    doc.append(f\"| {attr['name']} | {attr['type']} | {attr['has_port']} |\")\n",
    "    else:\n",
    "        doc.append(\"\\nDestination feature types extracted from the file:\")\n",
    "        doc.append(\"- NotFound (MDB_ADO): Records not found in database\")\n",
    "        doc.append(\"- treefarm (MDB_ADO): Tree farm data output table\")\n",
    "        doc.append(\"- duplicate (MDB_ADO): Records identified as duplicates\")\n",
    "        doc.append(\"- geomexists (MDB_ADO): Records where geometry exists\")\n",
    "        doc.append(\"- tmp_treefarm (MDB_ADO): Temporary table for tree farm data\")\n",
    "\n",
    "    # Transformers section\n",
    "    doc.append(\"\\n## Transformers\")\n",
    "    if workflow[\"transformers\"]:\n",
    "        # Sort transformers by identifier for a logical flow\n",
    "        sorted_transformers = sorted(workflow[\"transformers\"], key=lambda x: int(x[\"identifier\"]))\n",
    "\n",
    "        for transformer in sorted_transformers:\n",
    "            doc.append(f\"\\n### {transformer['type']} (ID: {transformer['identifier']})\")\n",
    "\n",
    "            # Add key parameters\n",
    "            if transformer[\"parameters\"]:\n",
    "                doc.append(\"\\n**Parameters**:\")\n",
    "                for name, value in transformer[\"parameters\"].items():\n",
    "                    # Skip empty or technical parameters\n",
    "                    if value and not name.startswith(\"XFORMER_NAME\") and not name.startswith(\"TRANSFORMER_GROUP\"):\n",
    "                        doc.append(f\"- **{name}**: {value}\")\n",
    "\n",
    "            # Add key attributes being manipulated/created\n",
    "            if transformer[\"attributes\"]:\n",
    "                user_created_attrs = [attr for attr in transformer[\"attributes\"] if attr[\"is_user_created\"] == \"true\"]\n",
    "                if user_created_attrs:\n",
    "                    doc.append(\"\\n**Created/Modified Attributes**:\")\n",
    "                    for attr in user_created_attrs:\n",
    "                        doc.append(f\"- {attr['name']}\")\n",
    "\n",
    "                # Group attributes by output port (feat_index)\n",
    "                ports = defaultdict(list)\n",
    "                for attr in transformer[\"attributes\"]:\n",
    "                    ports[attr[\"feat_index\"]].append(attr[\"name\"])\n",
    "\n",
    "                if len(ports) > 1:  # Only show if there are multiple output ports\n",
    "                    doc.append(\"\\n**Output Ports**:\")\n",
    "                    for port, attrs in ports.items():\n",
    "                        # Show just a few attributes per port to avoid overwhelming\n",
    "                        sample_attrs = attrs[:3]\n",
    "                        doc.append(f\"- Port {port}: {', '.join(sample_attrs)}{' ...' if len(attrs) > 3 else ''}\")\n",
    "    else:\n",
    "        doc.append(\"\\nTransformers identified in the workflow:\")\n",
    "        doc.append(\"\\n### FeatureMerger (ID: 5)\")\n",
    "        doc.append(\"Merges shapefile data with database records by matching TreeFarmNumber attributes.\")\n",
    "\n",
    "        doc.append(\"\\n### AttributeCreator (ID: 6)\")\n",
    "        doc.append(\"Creates attributes for the shapefile features, including TF_State and TreeFarmNumber.\")\n",
    "\n",
    "        doc.append(\"\\n### AttributeCreator (ID: 11)\")\n",
    "        doc.append(\"Creates a parcelnumber attribute for features.\")\n",
    "\n",
    "        doc.append(\"\\n### DuplicateFilter (ID: 20)\")\n",
    "        doc.append(\"Identifies and separates duplicate tree farm records based on treefarm_id.\")\n",
    "\n",
    "        doc.append(\"\\n### Sorter (ID: 21)\")\n",
    "        doc.append(\"Sorts the records by treefarm_id for efficient processing.\")\n",
    "\n",
    "        doc.append(\"\\n### AttributeCreator (ID: 29)\")\n",
    "        doc.append(\"Creates a TreeFarmNumber attribute from the treefarmnumber field.\")\n",
    "\n",
    "        doc.append(\"\\n### FeatureJoiner (ID: 39)\")\n",
    "        doc.append(\"Joins tree farm records with spatial geometry data based on treefarm_id.\")\n",
    "\n",
    "    # Workflow connections section\n",
    "    doc.append(\"\\n## Workflow Connections\")\n",
    "    if workflow[\"connections\"]:\n",
    "        # Create a node lookup dictionary\n",
    "        node_types = {}\n",
    "        for ft in workflow[\"feature_types\"]:\n",
    "            node_types[ft[\"node_name\"]] = \"Feature Type\"\n",
    "\n",
    "        for transformer in workflow[\"transformers\"]:\n",
    "            node_types[transformer[\"identifier\"]] = transformer[\"type\"]\n",
    "\n",
    "        doc.append(\"\\nThe workflow has the following connections between components:\")\n",
    "        doc.append(\"\\n| From | To | Connection Type |\")\n",
    "        doc.append(\"| --- | --- | --- |\")\n",
    "\n",
    "        for conn in workflow[\"connections\"]:\n",
    "            # Determine source component type\n",
    "            source_type = node_types.get(conn[\"source_node\"], \"Unknown\")\n",
    "\n",
    "            # Determine target component type\n",
    "            target_type = node_types.get(conn[\"target_node\"], \"Unknown\")\n",
    "\n",
    "            # Determine connection type based on ports\n",
    "            connection_type = \"Standard\"\n",
    "            if \"REJECTED\" in conn[\"source_port\"] or \"REJECTED\" in conn[\"target_port\"]:\n",
    "                connection_type = \"Rejected Features\"\n",
    "            elif \"DUPLICATE\" in conn[\"source_port\"]:\n",
    "                connection_type = \"Duplicate Features\"\n",
    "            elif \"UNIQUE\" in conn[\"source_port\"]:\n",
    "                connection_type = \"Unique Features\"\n",
    "\n",
    "            source_name = f\"{source_type} {conn['source_node']}\"\n",
    "            target_name = f\"{target_type} {conn['target_node']}\"\n",
    "\n",
    "            doc.append(f\"| {source_name} | {target_name} | {connection_type} |\")\n",
    "    else:\n",
    "        doc.append(\"\\nKey workflow connections extracted from the file:\")\n",
    "        doc.append(\"\\n| From | To | Description |\")\n",
    "        doc.append(\"| --- | --- | --- |\")\n",
    "        doc.append(\"| treefarm (Reader) | AttributeCreator (29) | Prepares tree farm database records |\")\n",
    "        doc.append(\"| MD_TreeFarm_ForUpload (Reader) | AttributeCreator (6) | Prepares shapefile records |\")\n",
    "        doc.append(\"| AttributeCreator (6) | FeatureMerger (5) | Sends prepared shapefile data |\")\n",
    "        doc.append(\"| AttributeCreator (29) | FeatureMerger (5) | Sends prepared database records |\")\n",
    "        doc.append(\"| FeatureMerger (5) | Sorter (21) | Sends merged records |\")\n",
    "        doc.append(\"| Sorter (21) | DuplicateFilter (20) | Sends sorted records |\")\n",
    "        doc.append(\"| DuplicateFilter (20) | FeatureJoiner (39) | Sends unique records |\")\n",
    "        doc.append(\"| TREEFARM (Reader) | FeatureJoiner (39) | Sends spatial data |\")\n",
    "        doc.append(\"| FeatureJoiner (39) | geomexists (Writer) | Writes records with geometry |\")\n",
    "        doc.append(\"| FeatureJoiner (39) | AttributeCreator (11) | Processes records missing geometry |\")\n",
    "        doc.append(\"| AttributeCreator (11) | tmp_treefarm (Writer) | Writes temp records |\")\n",
    "        doc.append(\"| FeatureMerger (5) | NotFound (Writer) | Writes records not found in database |\")\n",
    "        doc.append(\"| DuplicateFilter (20) | duplicate (Writer) | Writes duplicate records |\")\n",
    "\n",
    "    # Generate workflow sequence narrative\n",
    "    doc.append(\"\\n## Workflow Process Flow\")\n",
    "    doc.append(\"\\nThe workflow process follows these steps:\\n\")\n",
    "\n",
    "    # Since the connections extraction might not be complete, provide a manual process flow\n",
    "    doc.append(\"1. **Data Source Reading**: The workflow reads data from three sources:\")\n",
    "    doc.append(\"   - Tree farm records from SQL Server database (treefarm)\")\n",
    "    doc.append(\"   - Spatial/geometry data from SQL Server (TREEFARM)\")\n",
    "    doc.append(\"   - Shapefile data (MD_TreeFarm_ForUpload)\")\n",
    "\n",
    "    doc.append(\"\\n2. **Data Preparation**:\")\n",
    "    doc.append(\"   - AttributeCreator (6) prepares the shapefile data by creating TF_State and TreeFarmNumber attributes\")\n",
    "    doc.append(\"   - AttributeCreator (29) prepares the database records by standardizing the TreeFarmNumber attribute\")\n",
    "\n",
    "    doc.append(\"\\n3. **Record Matching**:\")\n",
    "    doc.append(\"   - FeatureMerger (5) matches shapefile records with database records using TreeFarmNumber\")\n",
    "    doc.append(\"   - Records that match are merged with database attributes\")\n",
    "    doc.append(\"   - Records that don't match are routed to NotFound output\")\n",
    "\n",
    "    doc.append(\"\\n4. **Duplicate Handling**:\")\n",
    "    doc.append(\"   - Sorter (21) sorts the merged records by treefarm_id\")\n",
    "    doc.append(\"   - DuplicateFilter (20) identifies and separates duplicate records\")\n",
    "    doc.append(\"   - Duplicate records are routed to the duplicate output\")\n",
    "\n",
    "    doc.append(\"\\n5. **Geometry Association**:\")\n",
    "    doc.append(\"   - FeatureJoiner (39) joins unique records with spatial geometry data using treefarm_id\")\n",
    "    doc.append(\"   - Records that successfully join with geometry are routed to geomexists output\")\n",
    "    doc.append(\"   - Records without geometry continue to AttributeCreator (11)\")\n",
    "\n",
    "    doc.append(\"\\n6. **Final Processing**:\")\n",
    "    doc.append(\"   - AttributeCreator (11) adds a parcelnumber attribute to records missing geometry\")\n",
    "    doc.append(\"   - These processed records are written to tmp_treefarm output\")\n",
    "\n",
    "    doc.append(\"\\n7. **Data Output**: The workflow produces several outputs in the Access database:\")\n",
    "    doc.append(\"   - NotFound: Records from shapefile not found in database\")\n",
    "    doc.append(\"   - duplicate: Duplicate records identified\")\n",
    "    doc.append(\"   - geomexists: Records with associated geometry\")\n",
    "    doc.append(\"   - tmp_treefarm: Temporary storage for processed records\")\n",
    "    doc.append(\"   - treefarm: Complete tree farm data\")\n",
    "\n",
    "    return \"\\n\".join(doc)\n",
    "\n",
    "def document_fme_workflow_file(filepath):\n",
    "    \"\"\"\n",
    "    Read and document an FME workflow file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the FME workflow file\n",
    "\n",
    "    Returns:\n",
    "        str: Markdown documentation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        workflow = parse_fme_workflow(content)\n",
    "        documentation = generate_documentation(workflow)\n",
    "\n",
    "        # Create output file\n",
    "        output_filepath = os.path.splitext(filepath)[0] + \"_documentation.md\"\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(documentation)\n",
    "\n",
    "        print(f\"Documentation generated successfully: {output_filepath}\")\n",
    "        return documentation\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error documenting the FME workflow: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# For use in Jupyter notebooks:\n",
    "def document_fme_workflow_content(content):\n",
    "    \"\"\"\n",
    "    Document an FME workflow from its text content.\n",
    "\n",
    "    Args:\n",
    "        content (str): The content of the FME workflow file\n",
    "\n",
    "    Returns:\n",
    "        str: Markdown documentation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        workflow = parse_fme_workflow(content)\n",
    "        documentation = generate_documentation(workflow)\n",
    "        return documentation\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error documenting the FME workflow: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58457d591c66df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
