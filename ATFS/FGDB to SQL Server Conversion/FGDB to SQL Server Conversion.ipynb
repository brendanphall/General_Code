{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"\"\"\n",
    "Windows FGDB to SQL Server Converter\n",
    "Optimized for Windows environment with direct SQL Server access\n",
    "\"\"\"\n",
    "\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "class WindowsFGDBConverter:\n",
    "    def __init__(self, server, database, fgdb_path):\n",
    "        self.server = server\n",
    "        self.database = database\n",
    "        self.fgdb_path = fgdb_path\n",
    "        self.engine = None\n",
    "\n",
    "    def create_connection(self):\n",
    "        \"\"\"Create SQL Server connection using Windows authentication\"\"\"\n",
    "        try:\n",
    "            # SQLAlchemy connection string for Windows auth\n",
    "            connection_string = f\"mssql+pyodbc://@{self.server}/{self.database}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    "\n",
    "            self.engine = create_engine(connection_string, fast_executemany=True)\n",
    "\n",
    "            # Test connection\n",
    "            with self.engine.connect() as conn:\n",
    "                result = conn.execute(text(\"SELECT @@VERSION\"))\n",
    "                version = result.fetchone()[0]\n",
    "                print(f\"âœ… Connected to SQL Server\")\n",
    "                print(f\"   Version: {version[:60]}...\")\n",
    "                return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Connection failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def list_fgdb_layers(self):\n",
    "        \"\"\"List all layers in the FGDB\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.fgdb_path):\n",
    "                print(f\"âŒ FGDB not found: {self.fgdb_path}\")\n",
    "                return []\n",
    "\n",
    "            layers = fiona.listlayers(self.fgdb_path)\n",
    "            print(f\"ğŸ“‹ Found {len(layers)} layers in FGDB:\")\n",
    "            for i, layer in enumerate(layers, 1):\n",
    "                print(f\"   {i}. {layer}\")\n",
    "\n",
    "            return layers\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error listing FGDB layers: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_layer_preview(self, layer_name, sample_size=5):\n",
    "        \"\"\"Get a preview of the layer data\"\"\"\n",
    "        try:\n",
    "            gdf = gpd.read_file(self.fgdb_path, layer=layer_name, rows=sample_size)\n",
    "\n",
    "            info = {\n",
    "                'total_features': len(gpd.read_file(self.fgdb_path, layer=layer_name, rows=1000).index),  # Quick count\n",
    "                'geometry_type': gdf.geometry.geom_type.mode().iloc[0] if len(gdf) > 0 else 'Unknown',\n",
    "                'columns': list(gdf.columns),\n",
    "                'crs': gdf.crs.to_string() if gdf.crs else 'Unknown',\n",
    "                'bounds': gdf.total_bounds if len(gdf) > 0 else None\n",
    "            }\n",
    "\n",
    "            return info, gdf.head(sample_size)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Error getting preview: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def clean_column_names(self, columns):\n",
    "        \"\"\"Clean column names for SQL Server compatibility\"\"\"\n",
    "        cleaned = []\n",
    "        for col in columns:\n",
    "            # Replace problematic characters\n",
    "            clean_col = col.replace(' ', '_').replace('-', '_').replace('.', '_')\n",
    "            clean_col = clean_col.replace('(', '').replace(')', '')\n",
    "            clean_col = clean_col.replace('#', 'num').replace('%', 'pct')\n",
    "\n",
    "            # Ensure it starts with a letter\n",
    "            if clean_col[0].isdigit():\n",
    "                clean_col = f\"col_{clean_col}\"\n",
    "\n",
    "            # Limit length\n",
    "            if len(clean_col) > 50:\n",
    "                clean_col = clean_col[:50]\n",
    "\n",
    "            cleaned.append(clean_col)\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    def determine_sql_types(self, gdf):\n",
    "        \"\"\"Determine appropriate SQL Server data types\"\"\"\n",
    "        dtype_mapping = {}\n",
    "\n",
    "        for col in gdf.columns:\n",
    "            if col == 'geometry':\n",
    "                continue\n",
    "\n",
    "            if gdf[col].dtype == 'object':\n",
    "                # String column - check max length\n",
    "                max_len = gdf[col].astype(str).str.len().max()\n",
    "                if pd.isna(max_len) or max_len == 0:\n",
    "                    dtype_mapping[col] = 'NVARCHAR(100)'\n",
    "                elif max_len > 4000:\n",
    "                    dtype_mapping[col] = 'TEXT'\n",
    "                else:\n",
    "                    dtype_mapping[col] = f'NVARCHAR({min(int(max_len * 1.5), 4000)})'\n",
    "\n",
    "            elif gdf[col].dtype in ['int64', 'int32', 'int16']:\n",
    "                dtype_mapping[col] = 'INTEGER'\n",
    "\n",
    "            elif gdf[col].dtype in ['float64', 'float32']:\n",
    "                dtype_mapping[col] = 'FLOAT'\n",
    "\n",
    "            elif gdf[col].dtype == 'bool':\n",
    "                dtype_mapping[col] = 'BIT'\n",
    "\n",
    "            else:\n",
    "                dtype_mapping[col] = 'NVARCHAR(255)'  # Default\n",
    "\n",
    "        # Add geometry column\n",
    "        dtype_mapping['Shape'] = 'GEOMETRY'\n",
    "\n",
    "        return dtype_mapping\n",
    "\n",
    "    def convert_layer(self, layer_name, chunk_size=1000):\n",
    "        \"\"\"Convert a single layer from FGDB to SQL Server\"\"\"\n",
    "        print(f\"\\nğŸ”„ Converting layer: {layer_name}\")\n",
    "\n",
    "        try:\n",
    "            # Get layer preview\n",
    "            info, preview = self.get_layer_preview(layer_name)\n",
    "            if info:\n",
    "                print(f\"   ğŸ“Š Features: ~{info['total_features']}\")\n",
    "                print(f\"   ğŸ—ºï¸ Geometry: {info['geometry_type']}\")\n",
    "                print(f\"   ğŸ“‹ Columns: {len(info['columns'])}\")\n",
    "                print(f\"   ğŸŒ CRS: {info['crs']}\")\n",
    "\n",
    "            # Read full layer\n",
    "            print(f\"   ğŸ“– Reading layer data...\")\n",
    "            gdf = gpd.read_file(self.fgdb_path, layer=layer_name)\n",
    "\n",
    "            if len(gdf) == 0:\n",
    "                print(f\"   âš ï¸ Layer is empty, skipping\")\n",
    "                return True\n",
    "\n",
    "            # Clean column names\n",
    "            original_columns = list(gdf.columns)\n",
    "            cleaned_columns = self.clean_column_names(original_columns)\n",
    "            column_mapping = dict(zip(original_columns, cleaned_columns))\n",
    "            gdf = gdf.rename(columns=column_mapping)\n",
    "\n",
    "            # Handle geometry\n",
    "            if 'geometry' in gdf.columns:\n",
    "                print(f\"   ğŸ”§ Converting geometry to WKT...\")\n",
    "                gdf['Shape'] = gdf['geometry'].apply(lambda x: x.wkt if x and x.is_valid else None)\n",
    "                gdf = gdf.drop('geometry', axis=1)\n",
    "\n",
    "            # Determine data types\n",
    "            dtype_mapping = self.determine_sql_types(gdf)\n",
    "\n",
    "            print(f\"   ğŸ’¾ Writing {len(gdf)} records to SQL Server...\")\n",
    "\n",
    "            # Drop existing table if it exists\n",
    "            with self.engine.connect() as conn:\n",
    "                conn.execute(text(f\"IF OBJECT_ID('{layer_name}', 'U') IS NOT NULL DROP TABLE {layer_name}\"))\n",
    "                conn.commit()\n",
    "\n",
    "            # Write data in chunks\n",
    "            if len(gdf) > chunk_size:\n",
    "                print(f\"   ğŸ“¦ Writing in chunks of {chunk_size}...\")\n",
    "\n",
    "            gdf.to_sql(\n",
    "                name=layer_name,\n",
    "                con=self.engine,\n",
    "                if_exists='replace',\n",
    "                index=False,\n",
    "                dtype=dtype_mapping,\n",
    "                chunksize=chunk_size,\n",
    "                method='multi'\n",
    "            )\n",
    "\n",
    "            # Create spatial index\n",
    "            print(f\"   ğŸ—‚ï¸ Creating spatial index...\")\n",
    "\n",
    "            # Calculate bounding box from data\n",
    "            if info and info['bounds'] is not None:\n",
    "                bounds = info['bounds']\n",
    "                buffer = 0.1  # Add small buffer\n",
    "                bbox = f\"({bounds[0]-buffer}, {bounds[1]-buffer}, {bounds[2]+buffer}, {bounds[3]+buffer})\"\n",
    "            else:\n",
    "                bbox = \"(-180, -90, 180, 90)\"  # World extent as fallback\n",
    "\n",
    "            spatial_index_sql = f\"\"\"\n",
    "            CREATE SPATIAL INDEX SIDX_{layer_name}_Shape\n",
    "            ON {layer_name}(Shape)\n",
    "            USING GEOMETRY_GRID\n",
    "            WITH (BOUNDING_BOX = {bbox})\n",
    "            \"\"\"\n",
    "\n",
    "            with self.engine.connect() as conn:\n",
    "                conn.execute(text(spatial_index_sql))\n",
    "                conn.commit()\n",
    "\n",
    "            print(f\"   âœ… Successfully converted {layer_name}\")\n",
    "\n",
    "            # Show column mapping if names were changed\n",
    "            changed_cols = [(orig, new) for orig, new in column_mapping.items() if orig != new and orig != 'geometry']\n",
    "            if changed_cols:\n",
    "                print(f\"   ğŸ“ Column name changes:\")\n",
    "                for orig, new in changed_cols[:5]:  # Show first 5\n",
    "                    print(f\"      {orig} â†’ {new}\")\n",
    "                if len(changed_cols) > 5:\n",
    "                    print(f\"      ... and {len(changed_cols) - 5} more\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error converting {layer_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def convert_all_layers(self, selected_layers=None):\n",
    "        \"\"\"Convert all or selected layers\"\"\"\n",
    "        print(\"ğŸš€ Starting FGDB to SQL Server conversion\")\n",
    "        print(f\"ğŸ“ Source: {self.fgdb_path}\")\n",
    "        print(f\"ğŸ¯ Target: {self.server} -> {self.database}\")\n",
    "\n",
    "        # Test connection\n",
    "        if not self.create_connection():\n",
    "            return False\n",
    "\n",
    "        # Get layers\n",
    "        available_layers = self.list_fgdb_layers()\n",
    "        if not available_layers:\n",
    "            return False\n",
    "\n",
    "        # Determine which layers to convert\n",
    "        if selected_layers:\n",
    "            layers_to_convert = [l for l in selected_layers if l in available_layers]\n",
    "            print(f\"\\nğŸ¯ Converting {len(layers_to_convert)} selected layers\")\n",
    "        else:\n",
    "            layers_to_convert = available_layers\n",
    "            print(f\"\\nğŸ¯ Converting all {len(layers_to_convert)} layers\")\n",
    "\n",
    "        # Convert each layer\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "\n",
    "        for i, layer in enumerate(layers_to_convert, 1):\n",
    "            print(f\"\\n[{i}/{len(layers_to_convert)}] Processing: {layer}\")\n",
    "\n",
    "            if self.convert_layer(layer):\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "\n",
    "        # Summary\n",
    "        print(f\"\\nğŸ“Š Conversion Summary:\")\n",
    "        print(f\"   âœ… Successful: {successful}\")\n",
    "        print(f\"   âŒ Failed: {failed}\")\n",
    "        print(f\"   ğŸ“‹ Total: {len(layers_to_convert)}\")\n",
    "\n",
    "        if successful > 0:\n",
    "            print(f\"\\nğŸ‰ Conversion complete! You can now query your data:\")\n",
    "            print(f\"   Example: SELECT COUNT(*) FROM {layers_to_convert[0]};\")\n",
    "            print(f\"   Geometry: SELECT Name, Shape.STAsText() FROM {layers_to_convert[0]};\")\n",
    "\n",
    "        return failed == 0\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    # Configuration - UPDATE THESE VALUES\n",
    "    CONFIG = {\n",
    "        'server': '100.103.17.32,1433',\n",
    "        'database': 'SpatialTest',\n",
    "        'fgdb_path': r'D:\\arcgis\\clients\\sample.gdb'  # UPDATE THIS PATH\n",
    "    }\n",
    "\n",
    "    print(\"=== Windows FGDB to SQL Server Converter ===\")\n",
    "\n",
    "    # Validate FGDB path\n",
    "    if not os.path.exists(CONFIG['fgdb_path']):\n",
    "        print(f\"âŒ FGDB not found: {CONFIG['fgdb_path']}\")\n",
    "        print(\"Please update the fgdb_path in the CONFIG section\")\n",
    "\n",
    "        # Try to find common locations\n",
    "        possible_paths = [\n",
    "            r'D:\\arcgis\\clients',\n",
    "            r'C:\\arcgis\\clients',\n",
    "            r'D:\\data',\n",
    "            r'C:\\data'\n",
    "        ]\n",
    "\n",
    "        print(\"\\nLooking for .gdb files in common locations:\")\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                gdb_files = list(Path(path).glob('*.gdb'))\n",
    "                if gdb_files:\n",
    "                    print(f\"   Found .gdb files in {path}:\")\n",
    "                    for gdb in gdb_files[:5]:  # Show first 5\n",
    "                        print(f\"      {gdb}\")\n",
    "        return\n",
    "\n",
    "    # Create converter and run\n",
    "    converter = WindowsFGDBConverter(\n",
    "        server=CONFIG['server'],\n",
    "        database=CONFIG['database'],\n",
    "        fgdb_path=CONFIG['fgdb_path']\n",
    "    )\n",
    "\n",
    "    # Convert all layers\n",
    "    success = converter.convert_all_layers()\n",
    "\n",
    "    # Or convert specific layers:\n",
    "    # success = converter.convert_all_layers(['states', 'fedlandp'])\n",
    "\n",
    "    if success:\n",
    "        print(\"\\nğŸ† All conversions completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ Some conversions failed. Check the output above for details.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T19:44:03.238286Z",
     "start_time": "2025-07-07T19:43:59.596539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FGDB to SQL Server Conversion - Jupyter Notebook Version\n",
    "# Cell 1: Test Package Imports\n",
    "\n",
    "print(\"=== Testing Python Package Imports ===\\n\")\n",
    "\n",
    "# Test each package individually\n",
    "packages_status = {}\n",
    "\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    print(\"âœ… geopandas imported successfully\")\n",
    "    print(f\"   Version: {gpd.__version__}\")\n",
    "    packages_status['geopandas'] = True\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ geopandas import failed: {e}\")\n",
    "    packages_status['geopandas'] = False\n",
    "\n",
    "try:\n",
    "    import fiona\n",
    "    print(\"âœ… fiona imported successfully\")\n",
    "    print(f\"   Version: {fiona.__version__}\")\n",
    "    packages_status['fiona'] = True\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ fiona import failed: {e}\")\n",
    "    packages_status['fiona'] = False\n",
    "\n",
    "try:\n",
    "    import pyodbc\n",
    "    print(\"âœ… pyodbc imported successfully\")\n",
    "    print(f\"   Version: {pyodbc.version}\")\n",
    "    packages_status['pyodbc'] = True\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ pyodbc import failed: {e}\")\n",
    "    packages_status['pyodbc'] = False\n",
    "\n",
    "try:\n",
    "    from sqlalchemy import create_engine, __version__ as sqlalchemy_version\n",
    "    print(\"âœ… sqlalchemy imported successfully\")\n",
    "    print(f\"   Version: {sqlalchemy_version}\")\n",
    "    packages_status['sqlalchemy'] = True\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ sqlalchemy import failed: {e}\")\n",
    "    packages_status['sqlalchemy'] = False\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"âœ… pandas imported successfully\")\n",
    "    print(f\"   Version: {pd.__version__}\")\n",
    "    packages_status['pandas'] = True\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ pandas import failed: {e}\")\n",
    "    packages_status['pandas'] = False\n",
    "\n",
    "print(f\"\\nğŸ“Š Package Summary:\")\n",
    "for pkg, status in packages_status.items():\n",
    "    status_icon = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"   {status_icon} {pkg}\")\n",
    "\n",
    "all_packages_ok = all(packages_status.values())\n",
    "print(f\"\\nğŸ¯ All packages ready: {'Yes' if all_packages_ok else 'No'}\")\n",
    "\n",
    "if not all_packages_ok:\n",
    "    print(\"\\nğŸ”§ To install missing packages, run in a new cell:\")\n",
    "    print(\"!conda install geopandas pyodbc sqlalchemy fiona -c conda-forge -y\")\n",
    "    print(\"# or\")\n",
    "    print(\"!pip install geopandas pyodbc sqlalchemy fiona\")"
   ],
   "id": "94a16aa01ff8e153",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Python Package Imports ===\n",
      "\n",
      "âœ… geopandas imported successfully\n",
      "   Version: 1.1.1\n",
      "âœ… fiona imported successfully\n",
      "   Version: 1.10.1\n",
      "âœ… pyodbc imported successfully\n",
      "   Version: 5.2.0\n",
      "âœ… sqlalchemy imported successfully\n",
      "   Version: 2.0.41\n",
      "âœ… pandas imported successfully\n",
      "   Version: 2.3.1\n",
      "\n",
      "ğŸ“Š Package Summary:\n",
      "   âœ… geopandas\n",
      "   âœ… fiona\n",
      "   âœ… pyodbc\n",
      "   âœ… sqlalchemy\n",
      "   âœ… pandas\n",
      "\n",
      "ğŸ¯ All packages ready: Yes\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T19:50:55.001895Z",
     "start_time": "2025-07-07T19:50:54.583489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FGDB to SQL Server Setup Test - PyCharm Version\n",
    "# Run this script first to verify everything is working\n",
    "\n",
    "print(\"=== FGDB to SQL Server Setup Test ===\\n\")\n",
    "\n",
    "def test_imports():\n",
    "    \"\"\"Test if all required packages work\"\"\"\n",
    "    print(\"ğŸ“¦ Testing package imports...\")\n",
    "\n",
    "    try:\n",
    "        import geopandas as gpd\n",
    "        print(f\"âœ… geopandas {gpd.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ geopandas failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        import fiona\n",
    "        print(f\"âœ… fiona {fiona.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ fiona failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        import pyodbc\n",
    "        print(f\"âœ… pyodbc {pyodbc.version}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ pyodbc failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        from sqlalchemy import create_engine, __version__ as sqlalchemy_version\n",
    "        print(f\"âœ… sqlalchemy {sqlalchemy_version}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ sqlalchemy failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        print(f\"âœ… pandas {pd.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ pandas failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def find_fgdb_files():\n",
    "    \"\"\"Look for FGDB files in common locations\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    print(\"\\nğŸ” Searching for .gdb files...\")\n",
    "\n",
    "    search_paths = [\n",
    "        r'D:\\arcgis\\clients',\n",
    "        r'C:\\arcgis\\clients',\n",
    "        r'D:\\data',\n",
    "        r'C:\\data',\n",
    "        r'D:\\arcgis',\n",
    "        r'C:\\arcgis'\n",
    "    ]\n",
    "\n",
    "    found_gdbs = []\n",
    "\n",
    "    for search_path in search_paths:\n",
    "        if os.path.exists(search_path):\n",
    "            print(f\"   Checking {search_path}...\")\n",
    "            gdb_files = list(Path(search_path).glob('**/*.gdb'))\n",
    "            if gdb_files:\n",
    "                print(f\"   ğŸ“ Found {len(gdb_files)} .gdb files:\")\n",
    "                for gdb in gdb_files[:5]:  # Show first 5\n",
    "                    print(f\"      {gdb}\")\n",
    "                    found_gdbs.append(str(gdb))\n",
    "                if len(gdb_files) > 5:\n",
    "                    print(f\"      ... and {len(gdb_files) - 5} more\")\n",
    "\n",
    "    if not found_gdbs:\n",
    "        print(\"   âŒ No .gdb files found in common locations\")\n",
    "        print(\"   ğŸ’¡ Try looking in your specific data directories\")\n",
    "\n",
    "    return found_gdbs\n",
    "\n",
    "def test_fgdb_access(fgdb_path):\n",
    "    \"\"\"Test access to a specific FGDB\"\"\"\n",
    "    import os\n",
    "    import fiona\n",
    "\n",
    "    print(f\"\\nğŸ“‚ Testing FGDB: {fgdb_path}\")\n",
    "\n",
    "    if not os.path.exists(fgdb_path):\n",
    "        print(f\"   âŒ Path does not exist\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        layers = fiona.listlayers(fgdb_path)\n",
    "        print(f\"   âœ… Accessible - {len(layers)} layers found:\")\n",
    "        for i, layer in enumerate(layers, 1):\n",
    "            print(f\"      {i}. {layer}\")\n",
    "        return layers\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error accessing FGDB: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_sql_server():\n",
    "    \"\"\"Test SQL Server connection\"\"\"\n",
    "    import pyodbc\n",
    "\n",
    "    print(f\"\\nğŸ”Œ Testing SQL Server connection...\")\n",
    "\n",
    "    server = \"100.103.17.32,1433\"\n",
    "    database = \"SpatialTest\"\n",
    "    username = \"dbeaver\"\n",
    "    password = \"dbeaver\"\n",
    "\n",
    "    try:\n",
    "        conn_str = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};\"\n",
    "        conn = pyodbc.connect(conn_str, timeout=10)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(\"SELECT @@VERSION\")\n",
    "        version = cursor.fetchone()[0]\n",
    "        print(f\"   âœ… Connected to SQL Server\")\n",
    "        print(f\"   ğŸ“Š Version: {version[:60]}...\")\n",
    "\n",
    "        cursor.execute(\"SELECT DB_NAME()\")\n",
    "        db_name = cursor.fetchone()[0]\n",
    "        print(f\"   ğŸ’¾ Database: {db_name}\")\n",
    "\n",
    "        conn.close()\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Connection failed: {e}\")\n",
    "        print(f\"   ğŸ’¡ Common issues:\")\n",
    "        print(f\"      - SQL Server not running\")\n",
    "        print(f\"      - Firewall blocking connection\")\n",
    "        print(f\"      - Windows authentication not enabled\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all tests\"\"\"\n",
    "\n",
    "    # Test 1: Package imports\n",
    "    if not test_imports():\n",
    "        print(\"\\nâŒ Package import test failed!\")\n",
    "        return\n",
    "\n",
    "    # Test 2: Find FGDB files\n",
    "    found_gdbs = find_fgdb_files()\n",
    "\n",
    "    # Test 3: Test specific FGDB if found\n",
    "    if found_gdbs:\n",
    "        test_fgdb = found_gdbs[0]  # Test first one found\n",
    "        layers = test_fgdb_access(test_fgdb)\n",
    "    else:\n",
    "        # Try common test path\n",
    "        test_fgdb = r\"Z:\\Users\\brendanhall\\GitHub\\General_Code\\ATFS\\FGDB to SQL Server Conversion\\esri_ref_data.gdb\\esri_ref_data.gdb\"\n",
    "        layers = test_fgdb_access(test_fgdb)\n",
    "\n",
    "    # Test 4: SQL Server connection\n",
    "    sql_ok = test_sql_server()\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“‹ TEST SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"âœ… Python packages: Ready\")\n",
    "    print(f\"{'âœ…' if found_gdbs or layers else 'âŒ'} FGDB access: {'Ready' if found_gdbs or layers else 'Check paths'}\")\n",
    "    print(f\"{'âœ…' if sql_ok else 'âŒ'} SQL Server: {'Ready' if sql_ok else 'Check connection'}\")\n",
    "\n",
    "    if (found_gdbs or layers) and sql_ok:\n",
    "        print(f\"\\nğŸ‰ ALL TESTS PASSED! Ready for FGDB conversion.\")\n",
    "        print(f\"\\nğŸ“ Next steps:\")\n",
    "        print(f\"   1. Update CONFIG in the main script with your FGDB path:\")\n",
    "        if found_gdbs:\n",
    "            print(f\"      'fgdb_path': r'{found_gdbs[0]}'\")\n",
    "        print(f\"   2. Run the main conversion script\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ Some tests failed. Please resolve issues before conversion.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "327643b386812352",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FGDB to SQL Server Setup Test ===\n",
      "\n",
      "ğŸ“¦ Testing package imports...\n",
      "âœ… geopandas 1.1.1\n",
      "âœ… fiona 1.10.1\n",
      "âœ… pyodbc 5.2.0\n",
      "âœ… sqlalchemy 2.0.41\n",
      "âœ… pandas 2.3.1\n",
      "\n",
      "ğŸ” Searching for .gdb files...\n",
      "   âŒ No .gdb files found in common locations\n",
      "   ğŸ’¡ Try looking in your specific data directories\n",
      "\n",
      "ğŸ“‚ Testing FGDB: Z:\\Users\\brendanhall\\GitHub\\General_Code\\ATFS\\FGDB to SQL Server Conversion\\esri_ref_data.gdb\\esri_ref_data.gdb\n",
      "   âœ… Accessible - 4 layers found:\n",
      "      1. states\n",
      "      2. fedlandp\n",
      "      3. park_dtl\n",
      "      4. dtl_st\n",
      "\n",
      "ğŸ”Œ Testing SQL Server connection...\n",
      "   âœ… Connected to SQL Server\n",
      "   ğŸ“Š Version: Microsoft SQL Server 2022 (RTM-GDR) (KB5046861) - 16.0.1135....\n",
      "   ğŸ’¾ Database: SpatialTest\n",
      "\n",
      "==================================================\n",
      "ğŸ“‹ TEST SUMMARY\n",
      "==================================================\n",
      "âœ… Python packages: Ready\n",
      "âœ… FGDB access: Ready\n",
      "âœ… SQL Server: Ready\n",
      "\n",
      "ğŸ‰ ALL TESTS PASSED! Ready for FGDB conversion.\n",
      "\n",
      "ğŸ“ Next steps:\n",
      "   1. Update CONFIG in the main script with your FGDB path:\n",
      "   2. Run the main conversion script\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T19:44:35.876209Z",
     "start_time": "2025-07-07T19:44:34.776637Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7260ca032cec1854",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FGDB to SQL Server Setup Test ===\n",
      "\n",
      "ğŸ“¦ Testing package imports...\n",
      "âœ… geopandas 1.1.1\n",
      "âœ… fiona 1.10.1\n",
      "âœ… pyodbc 5.2.0\n",
      "âœ… sqlalchemy 2.0.41\n",
      "âœ… pandas 2.3.1\n",
      "\n",
      "ğŸ” Searching for .gdb files...\n",
      "   âŒ No .gdb files found in common locations\n",
      "   ğŸ’¡ Try looking in your specific data directories\n",
      "\n",
      "ğŸ“‚ Testing FGDB: D:\\arcgis\\clients\\sample.gdb\n",
      "   âŒ Path does not exist\n",
      "\n",
      "ğŸ”Œ Testing SQL Server connection...\n",
      "   âŒ Connection failed: ('28000', '[28000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Login failed. The login is from an untrusted domain and cannot be used with Integrated authentication. (18452) (SQLDriverConnect); [28000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Login failed. The login is from an untrusted domain and cannot be used with Integrated authentication. (18452)')\n",
      "   ğŸ’¡ Common issues:\n",
      "      - SQL Server not running\n",
      "      - Firewall blocking connection\n",
      "      - Windows authentication not enabled\n",
      "\n",
      "==================================================\n",
      "ğŸ“‹ TEST SUMMARY\n",
      "==================================================\n",
      "âœ… Python packages: Ready\n",
      "âŒ FGDB access: Check paths\n",
      "âŒ SQL Server: Check connection\n",
      "\n",
      "âš ï¸ Some tests failed. Please resolve issues before conversion.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:07:54.708155Z",
     "start_time": "2025-07-07T20:04:01.337435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "FGDB to SQL Server Converter - FIXED VERSION\n",
    "Complete conversion script for ESRI File Geodatabase to SQL Server\n",
    "\"\"\"\n",
    "\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - UPDATE THESE VALUES\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    'server': '100.103.17.32,1433',\n",
    "    'database': 'SpatialTest',\n",
    "    'username': 'dbeaver',\n",
    "    'password': 'dbeaver',\n",
    "    'fgdb_path': r'Z:\\Users\\brendanhall\\GitHub\\General_Code\\ATFS\\FGDB to SQL Server Conversion\\esri_ref_data.gdb\\esri_ref_data.gdb',\n",
    "    'chunk_size': 1000,\n",
    "    'selected_layers': None  # None = all layers, or ['layer1', 'layer2'] for specific layers\n",
    "}\n",
    "\n",
    "class FGDBConverter:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.engine = None\n",
    "\n",
    "    def log(self, message, level=\"INFO\"):\n",
    "        \"\"\"Simple logging function\"\"\"\n",
    "        print(f\"[{level}] {message}\")\n",
    "\n",
    "    def setup_connection(self):\n",
    "        \"\"\"Create SQL Server connection\"\"\"\n",
    "        self.log(\"Setting up SQL Server connection...\")\n",
    "\n",
    "        try:\n",
    "            connection_string = (\n",
    "                f\"mssql+pyodbc://{self.config['username']}:{self.config['password']}\"\n",
    "                f\"@{self.config['server']}/{self.config['database']}\"\n",
    "                f\"?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "            )\n",
    "\n",
    "            self.engine = create_engine(\n",
    "                connection_string,\n",
    "                fast_executemany=True,\n",
    "                pool_pre_ping=True\n",
    "            )\n",
    "\n",
    "            # Test connection\n",
    "            with self.engine.connect() as conn:\n",
    "                result = conn.execute(text(\"SELECT @@VERSION, DB_NAME()\"))\n",
    "                version, db_name = result.fetchone()\n",
    "                self.log(f\"âœ… Connected to SQL Server\")\n",
    "                self.log(f\"   Database: {db_name}\")\n",
    "                self.log(f\"   Version: {version[:50]}...\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log(f\"âŒ Connection failed: {e}\", \"ERROR\")\n",
    "            return False\n",
    "\n",
    "    def get_fgdb_layers(self):\n",
    "        \"\"\"Get list of layers in FGDB\"\"\"\n",
    "        self.log(f\"Reading FGDB: {self.config['fgdb_path']}\")\n",
    "\n",
    "        if not os.path.exists(self.config['fgdb_path']):\n",
    "            self.log(f\"âŒ FGDB not found: {self.config['fgdb_path']}\", \"ERROR\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            layers = fiona.listlayers(self.config['fgdb_path'])\n",
    "            self.log(f\"âœ… Found {len(layers)} layers: {', '.join(layers)}\")\n",
    "            return layers\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log(f\"âŒ Error reading FGDB: {e}\", \"ERROR\")\n",
    "            return []\n",
    "\n",
    "    def clean_column_names(self, columns):\n",
    "        \"\"\"Clean column names for SQL Server compatibility\"\"\"\n",
    "        cleaned = []\n",
    "        for col in columns:\n",
    "            # Replace problematic characters\n",
    "            clean_col = col.replace(' ', '_').replace('-', '_').replace('.', '_')\n",
    "            clean_col = clean_col.replace('(', '').replace(')', '')\n",
    "            clean_col = clean_col.replace('#', 'num').replace('%', 'pct')\n",
    "            clean_col = clean_col.replace('/', '_').replace('\\\\', '_')\n",
    "\n",
    "            # Ensure starts with letter\n",
    "            if clean_col and clean_col[0].isdigit():\n",
    "                clean_col = f\"col_{clean_col}\"\n",
    "\n",
    "            # Limit length\n",
    "            if len(clean_col) > 50:\n",
    "                clean_col = clean_col[:50]\n",
    "\n",
    "            # Handle empty names\n",
    "            if not clean_col:\n",
    "                clean_col = f\"col_{len(cleaned)}\"\n",
    "\n",
    "            cleaned.append(clean_col)\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    def convert_layer(self, layer_name):\n",
    "        \"\"\"FIXED: Simplified conversion that lets pandas handle data types\"\"\"\n",
    "        self.log(f\"\\nğŸ”„ Converting layer: {layer_name}\")\n",
    "\n",
    "        try:\n",
    "            # Read the data\n",
    "            gdf = gpd.read_file(self.config['fgdb_path'], layer=layer_name)\n",
    "\n",
    "            if len(gdf) == 0:\n",
    "                self.log(f\"   âš ï¸ Layer is empty, skipping\")\n",
    "                return True\n",
    "\n",
    "            self.log(f\"   ğŸ“Š Processing {len(gdf)} features\")\n",
    "\n",
    "            # Show geometry info\n",
    "            if 'geometry' in gdf.columns and len(gdf) > 0:\n",
    "                geom_types = gdf.geometry.geom_type.value_counts()\n",
    "                self.log(f\"   ğŸ—ºï¸ Geometry types: {dict(geom_types)}\")\n",
    "\n",
    "            # Clean column names\n",
    "            original_columns = list(gdf.columns)\n",
    "            cleaned_columns = self.clean_column_names(original_columns)\n",
    "            column_mapping = dict(zip(original_columns, cleaned_columns))\n",
    "\n",
    "            # Handle duplicate column names\n",
    "            if len(set(cleaned_columns)) != len(cleaned_columns):\n",
    "                self.log(f\"   âš ï¸ Warning: Duplicate column names after cleaning\", \"WARN\")\n",
    "                seen = set()\n",
    "                unique_cleaned = []\n",
    "                for col in cleaned_columns:\n",
    "                    if col in seen:\n",
    "                        counter = 1\n",
    "                        new_col = f\"{col}_{counter}\"\n",
    "                        while new_col in seen:\n",
    "                            counter += 1\n",
    "                            new_col = f\"{col}_{counter}\"\n",
    "                        unique_cleaned.append(new_col)\n",
    "                        seen.add(new_col)\n",
    "                    else:\n",
    "                        unique_cleaned.append(col)\n",
    "                        seen.add(col)\n",
    "                cleaned_columns = unique_cleaned\n",
    "                column_mapping = dict(zip(original_columns, cleaned_columns))\n",
    "\n",
    "            gdf = gdf.rename(columns=column_mapping)\n",
    "\n",
    "            # Handle geometry\n",
    "            has_geometry = False\n",
    "            if 'geometry' in gdf.columns:\n",
    "                self.log(f\"   ğŸ”§ Converting geometry to WKT...\")\n",
    "\n",
    "                def safe_wkt(geom):\n",
    "                    try:\n",
    "                        if geom is None or geom.is_empty:\n",
    "                            return None\n",
    "                        if not geom.is_valid:\n",
    "                            geom = geom.buffer(0)  # Try to fix invalid geometry\n",
    "                        return geom.wkt\n",
    "                    except:\n",
    "                        return None\n",
    "\n",
    "                gdf['Shape'] = gdf['geometry'].apply(safe_wkt)\n",
    "                gdf = gdf.drop('geometry', axis=1)\n",
    "                has_geometry = True\n",
    "\n",
    "                valid_geoms = gdf['Shape'].notna().sum()\n",
    "                self.log(f\"   âœ… Converted {valid_geoms}/{len(gdf)} geometries to WKT\")\n",
    "\n",
    "            # Drop existing table\n",
    "            self.log(f\"   ğŸ—‘ï¸ Dropping existing table if exists...\")\n",
    "            with self.engine.connect() as conn:\n",
    "                conn.execute(text(f\"IF OBJECT_ID('{layer_name}', 'U') IS NOT NULL DROP TABLE {layer_name}\"))\n",
    "                conn.commit()\n",
    "\n",
    "            # Write without specifying dtypes - let pandas decide\n",
    "            self.log(f\"   ğŸ’¾ Writing {len(gdf)} records to SQL Server...\")\n",
    "            gdf.to_sql(\n",
    "                name=layer_name,\n",
    "                con=self.engine,\n",
    "                if_exists='replace',\n",
    "                index=False,\n",
    "                chunksize=self.config['chunk_size']\n",
    "                # No dtype parameter - let pandas auto-detect\n",
    "            )\n",
    "\n",
    "            # Fix geometry column type manually if we have geometry\n",
    "            if has_geometry:\n",
    "                self.log(f\"   ğŸ”§ Converting Shape column to GEOMETRY type...\")\n",
    "                try:\n",
    "                    with self.engine.connect() as conn:\n",
    "                        # Add a new geometry column\n",
    "                        conn.execute(text(f\"ALTER TABLE {layer_name} ADD Shape_Geom GEOMETRY\"))\n",
    "\n",
    "                        # Update with geometry data\n",
    "                        conn.execute(text(f\"\"\"\n",
    "                            UPDATE {layer_name}\n",
    "                            SET Shape_Geom = geometry::STGeomFromText(Shape, 4326)\n",
    "                            WHERE Shape IS NOT NULL AND Shape != ''\n",
    "                        \"\"\"))\n",
    "\n",
    "                        # Drop old text column and rename\n",
    "                        conn.execute(text(f\"ALTER TABLE {layer_name} DROP COLUMN Shape\"))\n",
    "                        conn.execute(text(f\"EXEC sp_rename '{layer_name}.Shape_Geom', 'Shape', 'COLUMN'\"))\n",
    "                        conn.commit()\n",
    "\n",
    "                        # Create spatial index\n",
    "                        self.log(f\"   ğŸ—‚ï¸ Creating spatial index...\")\n",
    "                        spatial_index_sql = f\"\"\"\n",
    "                        CREATE SPATIAL INDEX SIDX_{layer_name}_Shape\n",
    "                        ON {layer_name}(Shape)\n",
    "                        USING GEOMETRY_GRID\n",
    "                        WITH (BOUNDING_BOX = (-180, -90, 180, 90))\n",
    "                        \"\"\"\n",
    "                        conn.execute(text(spatial_index_sql))\n",
    "                        conn.commit()\n",
    "                        self.log(f\"   âœ… Spatial index created\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.log(f\"   âš ï¸ Warning: Could not create geometry column or spatial index: {e}\", \"WARN\")\n",
    "\n",
    "            # Show column mapping changes\n",
    "            changed_cols = [(orig, new) for orig, new in column_mapping.items()\n",
    "                           if orig != new and orig != 'geometry']\n",
    "            if changed_cols:\n",
    "                self.log(f\"   ğŸ“ Column name changes:\")\n",
    "                for orig, new in changed_cols[:5]:  # Show first 5\n",
    "                    self.log(f\"      {orig} â†’ {new}\")\n",
    "                if len(changed_cols) > 5:\n",
    "                    self.log(f\"      ... and {len(changed_cols) - 5} more\")\n",
    "\n",
    "            self.log(f\"   âœ… Successfully converted {layer_name}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log(f\"   âŒ Error converting {layer_name}: {e}\", \"ERROR\")\n",
    "            import traceback\n",
    "            self.log(f\"   Details: {traceback.format_exc()}\", \"ERROR\")\n",
    "            return False\n",
    "\n",
    "    def run_conversion(self):\n",
    "        \"\"\"Main conversion process\"\"\"\n",
    "        self.log(\"ğŸš€ Starting FGDB to SQL Server conversion\")\n",
    "        self.log(f\"ğŸ“ Source: {self.config['fgdb_path']}\")\n",
    "        self.log(f\"ğŸ¯ Target: {self.config['server']} -> {self.config['database']}\")\n",
    "\n",
    "        # Setup connection\n",
    "        if not self.setup_connection():\n",
    "            return False\n",
    "\n",
    "        # Get layers\n",
    "        available_layers = self.get_fgdb_layers()\n",
    "        if not available_layers:\n",
    "            return False\n",
    "\n",
    "        # Determine which layers to convert\n",
    "        if self.config['selected_layers']:\n",
    "            layers_to_convert = [l for l in self.config['selected_layers'] if l in available_layers]\n",
    "            missing_layers = [l for l in self.config['selected_layers'] if l not in available_layers]\n",
    "            if missing_layers:\n",
    "                self.log(f\"âš ï¸ Selected layers not found: {missing_layers}\", \"WARN\")\n",
    "        else:\n",
    "            layers_to_convert = available_layers\n",
    "\n",
    "        self.log(f\"ğŸ“‹ Converting {len(layers_to_convert)} layers: {', '.join(layers_to_convert)}\")\n",
    "\n",
    "        # Convert each layer\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "\n",
    "        for i, layer in enumerate(layers_to_convert, 1):\n",
    "            self.log(f\"\\n[{i}/{len(layers_to_convert)}] Processing: {layer}\")\n",
    "\n",
    "            if self.convert_layer(layer):\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "\n",
    "        # Final summary\n",
    "        self.log(f\"\\n\" + \"=\"*60)\n",
    "        self.log(f\"ğŸ“Š CONVERSION SUMMARY\")\n",
    "        self.log(f\"=\"*60)\n",
    "        self.log(f\"âœ… Successful: {successful}\")\n",
    "        self.log(f\"âŒ Failed: {failed}\")\n",
    "        self.log(f\"ğŸ“‹ Total: {len(layers_to_convert)}\")\n",
    "\n",
    "        if successful > 0:\n",
    "            self.log(f\"\\nğŸ‰ Conversion completed!\")\n",
    "            self.log(f\"ğŸ” Test your data with these SQL queries:\")\n",
    "            self.log(f\"   SELECT COUNT(*) FROM {layers_to_convert[0]};\")\n",
    "            self.log(f\"   SELECT TOP 5 * FROM {layers_to_convert[0]};\")\n",
    "            self.log(f\"   SELECT TOP 5 *, Shape.STAsText() AS WKT FROM {layers_to_convert[0]};\")\n",
    "\n",
    "        return failed == 0\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"    FGDB to SQL Server Converter - FIXED VERSION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Show configuration\n",
    "    print(f\"ğŸ“‹ Configuration:\")\n",
    "    print(f\"   Server: {CONFIG['server']}\")\n",
    "    print(f\"   Database: {CONFIG['database']}\")\n",
    "    print(f\"   Username: {CONFIG['username']}\")\n",
    "    print(f\"   FGDB: {CONFIG['fgdb_path']}\")\n",
    "    print(f\"   Chunk size: {CONFIG['chunk_size']}\")\n",
    "    print(f\"   Selected layers: {CONFIG['selected_layers'] or 'All layers'}\")\n",
    "    print()\n",
    "\n",
    "    # Validate FGDB path\n",
    "    if not os.path.exists(CONFIG['fgdb_path']):\n",
    "        print(f\"âŒ FGDB not found: {CONFIG['fgdb_path']}\")\n",
    "        print(f\"ğŸ’¡ Please update CONFIG['fgdb_path'] to the correct location\")\n",
    "        return\n",
    "\n",
    "    # Create converter and run\n",
    "    converter = FGDBConverter(CONFIG)\n",
    "    success = converter.run_conversion()\n",
    "\n",
    "    if success:\n",
    "        print(f\"\\nğŸ† All conversions completed successfully!\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ Some conversions failed. Check the log output above.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "c09453b204d0566c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "    FGDB to SQL Server Converter - FIXED VERSION\n",
      "============================================================\n",
      "ğŸ“‹ Configuration:\n",
      "   Server: 100.103.17.32,1433\n",
      "   Database: SpatialTest\n",
      "   Username: dbeaver\n",
      "   FGDB: Z:\\Users\\brendanhall\\GitHub\\General_Code\\ATFS\\FGDB to SQL Server Conversion\\esri_ref_data.gdb\\esri_ref_data.gdb\n",
      "   Chunk size: 1000\n",
      "   Selected layers: All layers\n",
      "\n",
      "[INFO] ğŸš€ Starting FGDB to SQL Server conversion\n",
      "[INFO] ğŸ“ Source: Z:\\Users\\brendanhall\\GitHub\\General_Code\\ATFS\\FGDB to SQL Server Conversion\\esri_ref_data.gdb\\esri_ref_data.gdb\n",
      "[INFO] ğŸ¯ Target: 100.103.17.32,1433 -> SpatialTest\n",
      "[INFO] Setting up SQL Server connection...\n",
      "[INFO] âœ… Connected to SQL Server\n",
      "[INFO]    Database: SpatialTest\n",
      "[INFO]    Version: Microsoft SQL Server 2022 (RTM-GDR) (KB5046861) - ...\n",
      "[INFO] Reading FGDB: Z:\\Users\\brendanhall\\GitHub\\General_Code\\ATFS\\FGDB to SQL Server Conversion\\esri_ref_data.gdb\\esri_ref_data.gdb\n",
      "[INFO] âœ… Found 4 layers: states, fedlandp, park_dtl, dtl_st\n",
      "[INFO] ğŸ“‹ Converting 4 layers: states, fedlandp, park_dtl, dtl_st\n",
      "[INFO] \n",
      "[1/4] Processing: states\n",
      "[INFO] \n",
      "ğŸ”„ Converting layer: states\n",
      "[INFO]    ğŸ“Š Processing 51 features\n",
      "[INFO]    ğŸ—ºï¸ Geometry types: {'MultiPolygon': np.int64(51)}\n",
      "[INFO]    ğŸ”§ Converting geometry to WKT...\n",
      "[INFO]    âœ… Converted 51/51 geometries to WKT\n",
      "[INFO]    ğŸ—‘ï¸ Dropping existing table if exists...\n",
      "[INFO]    ğŸ’¾ Writing 51 records to SQL Server...\n",
      "[INFO]    ğŸ”§ Converting Shape column to GEOMETRY type...\n",
      "[INFO]    ğŸ—‚ï¸ Creating spatial index...\n",
      "[WARN]    âš ï¸ Warning: Could not create geometry column or spatial index: (pyodbc.ProgrammingError) ('42000', \"[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Table 'states' does not have a clustered primary key as required by the spatial index. Make sure that the primary key column exists on the table before creating a spatial index. (12008) (SQLExecDirectW)\")\n",
      "[SQL: \n",
      "                        CREATE SPATIAL INDEX SIDX_states_Shape\n",
      "                        ON states(Shape)\n",
      "                        USING GEOMETRY_GRID\n",
      "                        WITH (BOUNDING_BOX = (-180, -90, 180, 90))\n",
      "                        ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "[INFO]    âœ… Successfully converted states\n",
      "[INFO] \n",
      "[2/4] Processing: fedlandp\n",
      "[INFO] \n",
      "ğŸ”„ Converting layer: fedlandp\n",
      "[INFO]    ğŸ“Š Processing 3687 features\n",
      "[INFO]    ğŸ—ºï¸ Geometry types: {'MultiPolygon': np.int64(3687)}\n",
      "[INFO]    ğŸ”§ Converting geometry to WKT...\n",
      "[INFO]    âœ… Converted 3687/3687 geometries to WKT\n",
      "[INFO]    ğŸ—‘ï¸ Dropping existing table if exists...\n",
      "[INFO]    ğŸ’¾ Writing 3687 records to SQL Server...\n",
      "[INFO]    ğŸ”§ Converting Shape column to GEOMETRY type...\n",
      "[INFO]    ğŸ—‚ï¸ Creating spatial index...\n",
      "[WARN]    âš ï¸ Warning: Could not create geometry column or spatial index: (pyodbc.ProgrammingError) ('42000', \"[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Table 'fedlandp' does not have a clustered primary key as required by the spatial index. Make sure that the primary key column exists on the table before creating a spatial index. (12008) (SQLExecDirectW)\")\n",
      "[SQL: \n",
      "                        CREATE SPATIAL INDEX SIDX_fedlandp_Shape\n",
      "                        ON fedlandp(Shape)\n",
      "                        USING GEOMETRY_GRID\n",
      "                        WITH (BOUNDING_BOX = (-180, -90, 180, 90))\n",
      "                        ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "[INFO]    âœ… Successfully converted fedlandp\n",
      "[INFO] \n",
      "[3/4] Processing: park_dtl\n",
      "[INFO] \n",
      "ğŸ”„ Converting layer: park_dtl\n",
      "[INFO]    ğŸ“Š Processing 26307 features\n",
      "[INFO]    ğŸ—ºï¸ Geometry types: {'MultiPolygon': np.int64(26307)}\n",
      "[INFO]    ğŸ”§ Converting geometry to WKT...\n",
      "[INFO]    âœ… Converted 26307/26307 geometries to WKT\n",
      "[INFO]    ğŸ—‘ï¸ Dropping existing table if exists...\n",
      "[INFO]    ğŸ’¾ Writing 26307 records to SQL Server...\n",
      "[INFO]    ğŸ”§ Converting Shape column to GEOMETRY type...\n",
      "[INFO]    ğŸ—‚ï¸ Creating spatial index...\n",
      "[WARN]    âš ï¸ Warning: Could not create geometry column or spatial index: (pyodbc.ProgrammingError) ('42000', \"[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Table 'park_dtl' does not have a clustered primary key as required by the spatial index. Make sure that the primary key column exists on the table before creating a spatial index. (12008) (SQLExecDirectW)\")\n",
      "[SQL: \n",
      "                        CREATE SPATIAL INDEX SIDX_park_dtl_Shape\n",
      "                        ON park_dtl(Shape)\n",
      "                        USING GEOMETRY_GRID\n",
      "                        WITH (BOUNDING_BOX = (-180, -90, 180, 90))\n",
      "                        ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "[INFO]    âœ… Successfully converted park_dtl\n",
      "[INFO] \n",
      "[4/4] Processing: dtl_st\n",
      "[INFO] \n",
      "ğŸ”„ Converting layer: dtl_st\n",
      "[INFO]    ğŸ“Š Processing 51 features\n",
      "[INFO]    ğŸ—ºï¸ Geometry types: {'MultiPolygon': np.int64(51)}\n",
      "[INFO]    ğŸ”§ Converting geometry to WKT...\n",
      "[INFO]    âœ… Converted 51/51 geometries to WKT\n",
      "[INFO]    ğŸ—‘ï¸ Dropping existing table if exists...\n",
      "[INFO]    ğŸ’¾ Writing 51 records to SQL Server...\n",
      "[INFO]    ğŸ”§ Converting Shape column to GEOMETRY type...\n",
      "[INFO]    ğŸ—‚ï¸ Creating spatial index...\n",
      "[WARN]    âš ï¸ Warning: Could not create geometry column or spatial index: (pyodbc.ProgrammingError) ('42000', \"[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Table 'dtl_st' does not have a clustered primary key as required by the spatial index. Make sure that the primary key column exists on the table before creating a spatial index. (12008) (SQLExecDirectW)\")\n",
      "[SQL: \n",
      "                        CREATE SPATIAL INDEX SIDX_dtl_st_Shape\n",
      "                        ON dtl_st(Shape)\n",
      "                        USING GEOMETRY_GRID\n",
      "                        WITH (BOUNDING_BOX = (-180, -90, 180, 90))\n",
      "                        ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "[INFO]    âœ… Successfully converted dtl_st\n",
      "[INFO] \n",
      "============================================================\n",
      "[INFO] ğŸ“Š CONVERSION SUMMARY\n",
      "[INFO] ============================================================\n",
      "[INFO] âœ… Successful: 4\n",
      "[INFO] âŒ Failed: 0\n",
      "[INFO] ğŸ“‹ Total: 4\n",
      "[INFO] \n",
      "ğŸ‰ Conversion completed!\n",
      "[INFO] ğŸ” Test your data with these SQL queries:\n",
      "[INFO]    SELECT COUNT(*) FROM states;\n",
      "[INFO]    SELECT TOP 5 * FROM states;\n",
      "[INFO]    SELECT TOP 5 *, Shape.STAsText() AS WKT FROM states;\n",
      "\n",
      "ğŸ† All conversions completed successfully!\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
